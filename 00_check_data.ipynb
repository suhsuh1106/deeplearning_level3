{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "UB6qenKgVyIG"
      },
      "source": [
        "Copyright 2019 Google LLC\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at\n",
        "\n",
        "https://www.apache.org/licenses/LICENSE-2.0\n",
        "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "CpPQDKFaVyII"
      },
      "source": [
        "Make sure that jupyter is installed by running below command (it will allow to create folders in user dir):\n",
        "\n",
        "```shell\n",
        "pip install jupyter --user\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "dGYS0NKeVyIJ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fatal: destination path 'google-research' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/google-research/google-research.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "LutDR7qoVyIQ"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "import tarfile\n",
        "import urllib\n",
        "sys.path.append('./google-research')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "t8e_WfgaVyIV"
      },
      "outputs": [],
      "source": [
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"  # no need to use gpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "2vxfxt-VVyIa"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-02-20 21:27:52.059901: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        },
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'tensorflow_model_optimization'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[8], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mlogging\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkws_streaming\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m \u001b[39mimport\u001b[39;00m model_flags\n\u001b[0;32m----> 5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkws_streaming\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m \u001b[39mimport\u001b[39;00m models\n\u001b[1;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkws_streaming\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlayers\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodes\u001b[39;00m \u001b[39mimport\u001b[39;00m Modes\n\u001b[1;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkws_streaming\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtrain\u001b[39;00m \u001b[39mimport\u001b[39;00m test\n",
            "File \u001b[0;32m~/code_ing/level3_assignment/deeplearning_level3/./google-research/kws_streaming/models/models.py:17\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# coding=utf-8\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m# Copyright 2022 The Google Research Authors.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[39m# limitations under the License.\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[39m\"\"\"Supported models.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mkws_streaming\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39matt_mh_rnn\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39matt_mh_rnn\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mkws_streaming\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39matt_rnn\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39matt_rnn\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mkws_streaming\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbc_resnet\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mbc_resnet\u001b[39;00m\n",
            "File \u001b[0;32m~/code_ing/level3_assignment/deeplearning_level3/./google-research/kws_streaming/models/att_mh_rnn.py:18\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[39m\"\"\"BiRNN model with multihead attention.\"\"\"\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkws_streaming\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlayers\u001b[39;00m \u001b[39mimport\u001b[39;00m modes\n\u001b[0;32m---> 18\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkws_streaming\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlayers\u001b[39;00m \u001b[39mimport\u001b[39;00m speech_features\n\u001b[1;32m     19\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkws_streaming\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlayers\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcompat\u001b[39;00m \u001b[39mimport\u001b[39;00m tf\n\u001b[1;32m     20\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mkws_streaming\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodel_utils\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mutils\u001b[39;00m\n",
            "File \u001b[0;32m~/code_ing/level3_assignment/deeplearning_level3/./google-research/kws_streaming/layers/speech_features.py:26\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkws_streaming\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlayers\u001b[39;00m \u001b[39mimport\u001b[39;00m random_shift\n\u001b[1;32m     25\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkws_streaming\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlayers\u001b[39;00m \u001b[39mimport\u001b[39;00m random_stretch_squeeze\n\u001b[0;32m---> 26\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkws_streaming\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlayers\u001b[39;00m \u001b[39mimport\u001b[39;00m spectrogram_augment\n\u001b[1;32m     27\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkws_streaming\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlayers\u001b[39;00m \u001b[39mimport\u001b[39;00m spectrogram_cutout\n\u001b[1;32m     28\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkws_streaming\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlayers\u001b[39;00m \u001b[39mimport\u001b[39;00m windowing\n",
            "File \u001b[0;32m~/code_ing/level3_assignment/deeplearning_level3/./google-research/kws_streaming/layers/spectrogram_augment.py:20\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mimport\u001b[39;00m Any, Dict\n\u001b[1;32m     19\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m control_flow_util\n\u001b[0;32m---> 20\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow_model_optimization\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtfmot\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkws_streaming\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlayers\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcompat\u001b[39;00m \u001b[39mimport\u001b[39;00m tf\n\u001b[1;32m     23\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m \u001b[39mimport\u001b[39;00m array_ops  \u001b[39m# pylint: disable=g-direct-tensorflow-import\u001b[39;00m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow_model_optimization'"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.compat.v1 as tf1\n",
        "import logging\n",
        "from kws_streaming.models import model_flags\n",
        "from kws_streaming.models import models\n",
        "from kws_streaming.layers.modes import Modes\n",
        "from kws_streaming.train import test\n",
        "from kws_streaming.models import utils\n",
        "from kws_streaming.data import input_data\n",
        "from kws_streaming.data import input_data_utils as du\n",
        "from kws_streaming.models import model_params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "jIkBmOt6VyIe"
      },
      "outputs": [],
      "source": [
        "tf1.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "sSETc8rdVyIl"
      },
      "outputs": [],
      "source": [
        "config = tf1.ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "sess = tf1.Session(config=config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "dnvm8YMlVyIq"
      },
      "outputs": [],
      "source": [
        "tf1.disable_eager_execution()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "yjvGL2IgVyIw"
      },
      "outputs": [],
      "source": [
        "DATA_VERSION = 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "smFll0s2VyI0"
      },
      "outputs": [],
      "source": [
        "current_dir = os.getcwd()\n",
        "\n",
        "if DATA_VERSION == 2:\n",
        "  DATA_URL = \"https://storage.googleapis.com/download.tensorflow.org/data/speech_commands_v0.02.tar.gz\"\n",
        "  DATA_PATH = os.path.join(current_dir, \"data2/\")\n",
        "elif DATA_VERSION == 1:\n",
        "  DATA_URL = \"http://download.tensorflow.org/data/speech_commands_v0.01.tar.gz\"\n",
        "  DATA_PATH = os.path.join(current_dir, \"data1/\")\n",
        "else:\n",
        "  assert(False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "2Ap7kbKPVyI4"
      },
      "outputs": [],
      "source": [
        "DATA_PATH"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "HCFAzvyfVyI8"
      },
      "outputs": [],
      "source": [
        "# create folder in current dir.\n",
        "# not data will be downloaded in DATA_PATH, feel free to specify your own DATA_PATH\n",
        "os.makedirs(DATA_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "1232Ifp3VyI_"
      },
      "outputs": [],
      "source": [
        "base_name = os.path.basename(DATA_URL)\n",
        "base_name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "Kf5YF4SsVyJD"
      },
      "outputs": [],
      "source": [
        "# it can take some time to download 2.3GB. After unpacking total size is 5.4GB\n",
        "arch_file_name = os.path.join(DATA_PATH, base_name)\n",
        "if not os.path.isfile(arch_file_name):\n",
        "  # download data\n",
        "  if sys.version_info >= (2, 5):\n",
        "    file_path = urllib.request.urlretrieve(DATA_URL, filename=arch_file_name)[0]\n",
        "  else:\n",
        "    file_path = urllib.urlretrieve(DATA_URL, filename=arch_file_name)[0]\n",
        "\n",
        "  # unpack it\n",
        "  file_name, file_extension = os.path.splitext(base_name)\n",
        "  tar = tarfile.open(file_path)\n",
        "  tar.extractall(DATA_PATH)\n",
        "  tar.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "T8z0-SnfVyJG"
      },
      "outputs": [],
      "source": [
        "# default parameters for data splitting\n",
        "flags = model_params.Params()\n",
        "flags.data_dir = DATA_PATH\n",
        "flags.data_url = DATA_URL\n",
        "flags = model_flags.update_flags(flags)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "TsRktp1mVyJK"
      },
      "outputs": [],
      "source": [
        "audio_processor = input_data.AudioProcessor(flags)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "ZXABxi76VyJO"
      },
      "outputs": [],
      "source": [
        "testing_set_size = audio_processor.set_size('testing')\n",
        "print(\"testing_set_size \" + str(testing_set_size))\n",
        "training_set_size = audio_processor.set_size('training')\n",
        "print(\"training_set_size \" + str(training_set_size))\n",
        "validation_set_size = audio_processor.set_size('validation')\n",
        "print(\"validation_set_size \" + str(validation_set_size))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "kDC0kbR2VyJR"
      },
      "outputs": [],
      "source": [
        "# V2\n",
        "# testing_set_size 4890\n",
        "# training_set_size 36923\n",
        "# validation_set_size 4445\n",
        "\n",
        "# V1\n",
        "# testing_set_size 3081\n",
        "# training_set_size 22246\n",
        "# validation_set_size 3093"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "RocIfpAtVyJU"
      },
      "outputs": [],
      "source": [
        "# all words used for modeling: we have target words + unknown words (with index 1)\n",
        "audio_processor.word_to_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "iea72Q5oVyJY"
      },
      "outputs": [],
      "source": [
        "# find the start of the file name where label begins\n",
        "string = audio_processor.data_index[\"validation\"][0]['file']\n",
        "res = [i for i in range(len(string)) if string.startswith('/', i)] \n",
        "start_file = res[-2]+1\n",
        "start_file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "DQWh0HF0VyJb"
      },
      "outputs": [],
      "source": [
        "audio_processor.data_index[\"validation\"][0]['file'][start_file:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "LdihI451VyJe"
      },
      "outputs": [],
      "source": [
        "index_to_label = {}\n",
        "unknown_category = []\n",
        "# labeles used for training\n",
        "for word in audio_processor.word_to_index.keys():\n",
        "  if audio_processor.word_to_index[word] == du.SILENCE_INDEX:\n",
        "    index_to_label[audio_processor.word_to_index[word]] = du.SILENCE_LABEL\n",
        "  elif audio_processor.word_to_index[word] == du.UNKNOWN_WORD_INDEX:\n",
        "    index_to_label[audio_processor.word_to_index[word]] = du.UNKNOWN_WORD_LABEL\n",
        "    unknown_category.append(word)\n",
        "  else:\n",
        "    index_to_label[audio_processor.word_to_index[word]] = word\n",
        "\n",
        "# training labels\n",
        "index_to_label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "vsgt_OpnVyJh"
      },
      "outputs": [],
      "source": [
        "# words belonging to unknown categry\n",
        "unknown_category"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "JDCSwc9SVyJk"
      },
      "outputs": [],
      "source": [
        "def get_distribution(mode):\n",
        "  distrib_label = {}\n",
        "  distrib_words = {}\n",
        "  files = {}\n",
        "  for data in audio_processor.data_index[mode]:\n",
        "    word = data['label']\n",
        "    file = data['file'][start_file:]\n",
        "    index = audio_processor.word_to_index[word]\n",
        "    label = index_to_label[index]\n",
        "    if word in files:\n",
        "      files[word].append(file)\n",
        "    else:\n",
        "      files[word] = [file]\n",
        "\n",
        "    if label in distrib_label:\n",
        "      distrib_label[label] = distrib_label[label] + 1\n",
        "    else:\n",
        "      distrib_label[label] = 1\n",
        "\n",
        "    if word in distrib_words:\n",
        "      distrib_words[word] = distrib_words[word] + 1\n",
        "    else:\n",
        "      distrib_words[word] = 1\n",
        "  return distrib_words, distrib_label, files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "fwrCIcC0VyJn"
      },
      "outputs": [],
      "source": [
        "# distribution of labeles in testing data\n",
        "distrib_words_testing, distrib_labels_testing, files_testing = get_distribution('testing')\n",
        "distrib_labels_testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "qG0cJ1ifVyJr"
      },
      "outputs": [],
      "source": [
        "# distribution of labeles in training data\n",
        "distrib_words_training, distrib_labels_training, files_training = get_distribution('training')\n",
        "distrib_labels_training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "GRsumUc0VyJv"
      },
      "outputs": [],
      "source": [
        "def parse_files(set_list_fname, label='yes'):\n",
        "  set_files = []\n",
        "  with open(set_list_fname) as f:\n",
        "    while True:\n",
        "      line = f.readline()\n",
        "      if not line:\n",
        "        break\n",
        "      if line.split('/')[0] == label:\n",
        "        set_files.append(line[:-1])\n",
        "  return set_files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "iHw_eD2rVyJy"
      },
      "outputs": [],
      "source": [
        "def validate(my_list1, list2, print_in_list2=False):\n",
        "  cnt_my_val2 = 0\n",
        "  cnt_my_val1 = 0\n",
        "  for my_val in my_list1:\n",
        "    if my_val in list2:\n",
        "      cnt_my_val2 = cnt_my_val2 + 1\n",
        "      if print_in_list2:\n",
        "        print(my_val)\n",
        "    else:\n",
        "      cnt_my_val1 = cnt_my_val1 + 1\n",
        "      if not print_in_list2:\n",
        "        print(my_val)\n",
        "  return cnt_my_val1, cnt_my_val2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "CSbC4wmmVyJ2"
      },
      "outputs": [],
      "source": [
        "file_list = os.path.join(DATA_PATH, \"testing_list.txt\")\n",
        "\n",
        "# validate that all wav used during testing belongs to testing_list\n",
        "for word in files_testing.keys():\n",
        "  if word != '_silence_':\n",
        "    word_files = parse_files(file_list, label=word)\n",
        "    _, cnt_val = validate(files_testing[word], word_files, False)\n",
        "    assert(cnt_val-len(files_testing[word])==0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "2Q8zKJWwVyJ4"
      },
      "outputs": [],
      "source": [
        "\n",
        "distrib_words_training, distrib_labels_training, files_training = get_distribution('training')\n",
        "\n",
        "# validate that all wav used during testing do not belong to training data\n",
        "for word in files_testing.keys():\n",
        "  if word != '_silence_': # silence file does not matter becasue it is multiplied by zero\n",
        "    word_files = files_testing[word]\n",
        "    _, cnt_val = validate(files_training[word], word_files, True)\n",
        "    assert(cnt_val == 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "nQqB8IqLVyJ7"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "00_check-data.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "kws",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "a3701394dc19f22072f6d626bca333cc88e54fd156b5ea1f3f8f310b3f04ace4"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
